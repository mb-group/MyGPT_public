{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#mygpt","title":"MyGPT","text":"<p>Note: This repository contains installation instructions for the MyGPT with Docker images and will not need source code. If you need access to MyGPT source to help in development process, please contact Jaimin Patel (Email: jaimin.patel@stjude.org) or appropriate person.</p> <p> Note2: This repository is for public use and will not contain any private information. If you need access to private repository, please contact Jaimin Patel</p> <p>ChatGPT has revolutionized creative occupations, but tasks requiring factual backing suffer from generalized models and limitations such as hallucinations and inconsistency. Here, we present MyGPT \u2014 an open-source Large Language Model (LLM) pipeline to ask questions for content from a curated list of publications or video/audio lectures. MyGPT minimizes hallucination by providing a context for the question and generates accurate answers with source citing. MyGPT can run on personal devices or cloud infrastructures and can help with complex tasks such as literature review and learning. </p>"},{"location":"#pipeline","title":"Pipeline","text":"<p>We have divided the MyGPT pipeline architecture into three sections:  1. User interface (UI): The UI is the front-end of the pipeline. It is a web application that allows users to interact with the pipeline. The UI is built using ReactJS. 2. Backend server: The backend server is responsible for handling requests from the UI and sending them to the LLM server. The backend server is built using Python Django. 3. LLM server: The LLM server is responsible for generating answers to the questions asked by the user. We are using Ollama for the LLM server.</p>"},{"location":"#issues","title":"Issues","text":"<p>If you come across any bug or error, please report it in the issues section.</p>"},{"location":"FAQs/","title":"FAQs","text":"<ul> <li>Asking questions</li> <li>MyGPT answers</li> </ul>"},{"location":"FAQs/#asking-questions","title":"Asking questions","text":""},{"location":"FAQs/#1-what-kind-of-questions-can-i-ask-mygpt","title":"1.  What kind of questions can I ask MyGPT?","text":"<p>During MyGPT evaluation, we have used six different question types as follows: \u2022   Keyword search: An acronym or a definition from the papers. For example, \u201cWhat is acetyl-CoA?\u201d \u2022   Summarization: Question that summarize a topic or method. For example, \u201cWhat is the proof that pathogenic gain-of-function mutations can shift the equilibrium towards the active state in FGFRs?\u201d \u2022   Yes/no: The answer can be a simple yes or no. For example, \u201cDoes the regulatory domain inhibit the PAK4 kinase activity?\u201d \u2022   Data query: Question to find data or other facts from a section or information from several papers. For example, \u201cWhere are the leukemia-related breakpoints located in CBP/p300?\u201d \u2022   Complex question: A research question to collect information from different parts of a paper and synthesize a response. For example, \u201cWhat are the biggest issues facing the development of better CAR-T therapies? Rank them in order of difficulty and provide examples with citations on all potential strategies to overcome these limitations.\u201d \u2022   Irrelevant question: A question about a topic that is not covered by the publication library. For example, asking questions about Harry Potter to a library of GPCR documents: \u201cWhat does Hagrid give Harry as a Christmas present?\u201d.  You can also ask questions that do not follow the above categories, and MyGPT will answer them if they are within the limits of LLMs and RAG concepts. Several tasks, such as performing statistical analysis, analyzing whole documents, or listing references from literature with high accuracy, are beyond the current capability of LLMs and MyGPT.</p>"},{"location":"FAQs/#2-can-i-ask-any-questions-about-topics-from-my-library","title":"2. Can I ask any questions about topics from my library?","text":"<p>You can ask questions that can be answered using the information available within your document library. If the data is missing from the documents, MyGPT will not be able to answer it or will let you know that the information is missing from your papers.</p>"},{"location":"FAQs/#3-can-i-ask-a-question-about-information-that-is-in-tables-or-figures-in-the-library-documents","title":"3. Can I ask a question about information that is in tables or figures in the library documents?","text":"<p>MyGPT will read tables as text and try to use that information to answer your question. However, it will not be able to parse the table and find the relationship between table columns and information from the table. Also, MyGPT will not perform statistical analysis for data present in the table. MyGPT can\u2019t find information from the figures if it can only be answered by interpreting it or by performing the statistical analysis, although it will be able to read the legend of the figure. If the question can be answered using the legend, it can answer it.</p>"},{"location":"FAQs/#mygpt-answers","title":"MyGPT answers","text":""},{"location":"FAQs/#4-what-information-does-mygpt-uses-to-answer-my-question","title":"4.  What information does MyGPT uses to answer my question?","text":"<p>MyGPT uses information in the documents in your library as the primary source of information. If the information is missing from the library, MyGPT will use inherent knowledge of LLM, which was used for the training phase of specific LLM.</p>"},{"location":"FAQs/#5-is-mygpt-using-the-internet-to-answer-my-question","title":"5. Is MyGPT using the internet to answer my question?","text":"<p>MyGPT does not use the internet or any external API services to get information to answer your question.</p>"},{"location":"FAQs/#6-can-i-use-mygpt-to-chat-with-llm-without-incorporating-documents-pipeline-rag-like-chatgpt","title":"6. Can I use MyGPT to chat with LLM without incorporating documents pipeline (RAG), like ChatGPT?","text":"<p>You can use MyGPT to chat directly with LLM without the RAG pipeline. Under the input box where users can ask questions, we have provided a switch to skip the RAG pipeline. In this case, MyGPT will use LLM\u2019s inherent knowledge to answer user\u2019s questions.</p> <p></p>"},{"location":"FAQs/#7-what-happens-if-the-information-to-answer-the-question-is-not-available-in-my-library","title":"7. What happens if the information to answer the question is not available in my library?","text":"<p>If the information to answer the question is unavailable in your library, MyGPT will use its inherent knowledge to answer it. It will also provide confidence matrices in the form of question relevance score (QRS), answer relevance score (ARS) and hallucination index (HI). Low QRS, QRS, and high HI should be interpreted as an indication to verify and cross-reference the generated answer with the retrieved context highlighted in the documents. Suppose the question is off-topic from the subjects covered in the papers. In that case, the QRS score will be zero, indicating the generated answer does not use any information from the document and is entirely generated using inherent knowledge of LLM.   </p>"},{"location":"FAQs/#8-if-i-am-certain-my-library-has-the-necessary-information-to-answer-the-question-but-mygpt-is-not-able-to-find-it-what-should-i-do","title":"8. If I am certain my library has the necessary information to answer the question, but MyGPT is not able to find it, what should I do?","text":"<p>We recommend several solutions if you are sure that the documents contain the information to answer the question.  1.  The first thing to try is rephrasing it and providing more context with your question if it\u2019s too short.  2.  You can also adjust QRS and ARS cut-off values from the customizations. Certain words in embedding models have a higher distance than our perceived understanding of language as the specific domain knowledge was lacking in training of embedding models. Increasing the QCworst and Aworst values can aid in finding the correct context from documents. 3.  We have also observed different embedding models provide different meanings to the same library of documents. So, if rephrasing and adjusting relevance scores doesn\u2019t work, we recommend creating the same library with a different embedding model.</p>"},{"location":"FAQs/#9-what-happens-if-the-information-to-answer-the-question-is-contained-in-a-figure-or-table","title":"9. What happens if the information to answer the question is contained in a figure or table?","text":"<p>MyGPT will read tables as text and try to use that information to answer your question. However, it will not be able to parse the table and find the relationship between table column names and the data from the table. Also, MyGPT will not perform statistical analysis for data present in the table. MyGPT will be able to read the legend of the figure. If the question can be answered using the legend, it can answer it. However, if your question can only be answered by interpreting the figure or by comparing data present in your figure, MyGPT will not be able to answer it.</p>"},{"location":"FAQs/#10-if-there-is-information-on-my-library-that-is-outdated-or-inconsistent-with-facts-available-in-public-domain-will-mygpt-detect-the-inconsistency","title":"10. If there is information on my library that is outdated or inconsistent with facts available in public domain, will MyGPT detect the inconsistency?","text":"<p>MyGPT is designed to perform question-answering in the context of your library of documents and will hold the information from your documents as the highest truth. If the information in your document is outdated compared to facts in the public domain, MyGPT will answer them using only information from your documents. If the LLM has more up-to-date information about your question, the answer relevance score (ARS) and hallucination index (HI) may be able to guide you. MyGPT also provides answers generated without the RAG pipeline as the drop-down with the original MyGPT-generated answers. You can compare that answer with an original answer to verify the discrepancy in relevance scores. However, if the most up-to-date information about your topic is also missing from LLM training data, MyGPT will answer it only using information from your documents.  </p> <p></p>"},{"location":"FAQs/#11-if-the-answer-is-related-to-up-to-date-information-that-is-contained-in-my-library-but-that-was-missing-from-the-training-data-used-for-the-llm-being-used-will-mygpt-be-able-to-answer-accurately","title":"11. If the answer is related to up-to-date information that is contained in my library but that was missing from the training data used for the LLM being used, will MyGPT be able to answer accurately?","text":"<p>Yes, MyGPT uses the facts present in your documents as the highest truth and will be able to use them as context to answer your question. MyGPT does not rely on LLM training data and eliminates the need for periodic retraining of LLMs with new information.</p>"},{"location":"FAQs/#12-is-mygpt-using-any-personal-information-about-me-as-the-knowledge-base","title":"12. Is MyGPT using any personal information about me as the knowledge base?","text":"<p>No, MyGPT does not use any personal information. It uses login information for hosted instances and it's independent of the RAG pipeline.</p>"},{"location":"FAQs/#13-is-there-chat-history-available-anywhere","title":"13. Is there chat history available anywhere?","text":"<p>Yes, MyGPT has a \u201cHistory\u201d menu to access chat history, which is available for any public library or your libraries after logging in.</p>"},{"location":"development/","title":"Developer's Guide","text":""},{"location":"development/#backend-database","title":"Backend database","text":"<p>The backend database is used to store the documents, questions, answers, and user information. The backend database is built using Django and PostgreSQL. The database can be accessed by creating a superuser and logging in to the admin panel.</p>"},{"location":"development/#create-super-user","title":"create super user","text":"<p>To create super user, run following command if you are using pre-built docker images:</p> <pre><code>cd MyGPT_public/installation/macOS/\nbash create_superuser.sh\n</code></pre> <p>You can check backend database at http://localhost:8000/admin/ with username and password you created in above step.</p>"},{"location":"development/#api-endpoints","title":"API Endpoints","text":"<p>These API endpoints are available for MyGPT backend server. This endpoint will be used for testing, evaluation and by the frontend to interact with the backend server.</p> <ul> <li>Get datasets</li> <li>Get documents</li> <li>Get context</li> <li>Save answer</li> <li>Get question details</li> <li>Get conversation history</li> <li>Frontend settings</li> </ul>"},{"location":"development/#get-datasets","title":"Get datasets","text":"<p>This endpoint will return the list of datasets available in the backend database. The response will be in JSON format.</p> <p>The endpoint is:</p> <pre><code>POST /api/get_datasets/\n</code></pre> <p>The request body will be:</p> <pre><code>{\n    \"user_email\": \"abc@xyz.com\",\n    \"user_group\": \"\"\n}\n</code></pre> <p>The response will be:</p> <pre><code>{\n    \"datasets\": [\n         {\n            \"dataset_name\": \"GPCR\",\n            \"zotero_id\": \"-\",\n            \"dataset_size\": 3366,\n            \"user\": \"Patel_Jaimin\",\n            \"user_email\": \"jpatel2@stjude.org\",\n            \"user_group\": \"user\",\n            \"embedding_added\": true,\n            \"embedding_model\": \"multi-qa-MiniLM-L6-cos-v1\",\n            \"chunksize\": 1000,\n            \"overlap\": true,\n            \"distance_function\": \"l2\",\n            \"direct_chat_without_docs\": false,\n            \"dataset_date_time\": \"2024-10-19T18:58:12.267Z\"\n        },\n        {\n            \"dataset_name\": \"Harry_Potter_book_1\",\n            \"zotero_id\": \"-\",\n            \"dataset_size\": 680,\n            \"user\": \"Patel_Jaimin\",\n            \"user_email\": \"jpatel2@stjude.org\",\n            \"user_group\": \"user\",\n            \"embedding_added\": true,\n            \"embedding_model\": \"multi-qa-MiniLM-L6-cos-v1\",\n            \"chunksize\": 1000,\n            \"overlap\": true,\n            \"distance_function\": \"l2\",\n            \"direct_chat_without_docs\": false,\n            \"dataset_date_time\": \"2024-10-19T19:15:07.298Z\"\n        },\n    ]\n}\n</code></pre>"},{"location":"development/#get-documents","title":"Get documents","text":"<p>This endpoint will return the list of documents available for a specific dataset. The response will be in JSON format.</p> <p>The endpoint is:  <code>GET /api/get_documents/?dataset=IDR</code></p> <p>The response will be:</p> <p><code>json     {     \"documents\": [         {             \"paper_title\": \"Attributes of short linear motifs\",             \"paper_attachment\": \"papers/IDR-cosine/paper1.pdf\",             \"highlighted_attachment\": \"-\",             \"paper_dataset\": 239,             \"paper_date_time\": \"2024-07-26T20:43:05.553Z\"         },         {             \"paper_title\": \"Conditionally and Transiently Disordered Proteins- Awakening Cryptic Disorder To Regulate Protein Function\",             \"paper_attachment\": \"papers/IDR-cosine/paper2.pdf\",             \"highlighted_attachment\": \"-\",             \"paper_dataset\": 239,             \"paper_date_time\": \"2024-07-26T20:43:05.834Z\"         }     ] }</code></p>"},{"location":"development/#get-context","title":"Get context","text":"<p>This endpoint will return the context (chunks) from the document to answer the user provided question. The response will be in JSON format.</p> <p>The endpoint is:</p> <pre><code>POST /api/get_context/\n</code></pre> <p>The request body will be:</p> <pre><code>{\n    \"text\": \"Which cohorts are available from Survivorship portal?\",\n    \"model_type\": \"llama3:latest\",\n    \"dataset\": \"Survivorship\",\n    \"new_conversation\": true,\n    \"related_query\": false,\n    \"previous_query\": \"\",\n    \"no_context\": false,\n    \"use_default_qrs\": true,\n    \"question_best_distance\": 0.2,\n    \"question_worst_distance\": 1.7\n}\n</code></pre> <p>The response will be:</p> <pre><code>{\n    \"context\": \"ding individual survivor data for offline analysis. These features will enable on-demand access and open-ended exploration, allowing the St. Jude Survivorship Portal to serve as a valuable resource for the survivorship research community and beyond. Resul ts Overview of the Portal The St. Jude Survivorship Portal is a free, open-access data portal that hosts data from two childhood cancer survivor - ship cohorts: the SJLIFE and the CCSS (Fig. 1A; Supple- mentary Fig. S1; Supplementary Table S1). A total of 5,053 SJLIFE survivors and 2,688 CCSS survivors were included in the portal based on inclusion criteria described in \u201cMethods.\u201d Cohort phenotypic data consist of variables ranging from demographics, cancer diagnoses, treatments, clinical assess- ments, graded chronic health conditions, and self-reported symptoms of survivors with up to several decades of follow up (Supplementary Table S2). We have organized these data into a hierarchical data dictionary that users can easily nare avail- able through either the data dictionary or genome browser on the portal. Navigation through the St. Jude Survivorship Portal is guided by four main tabs: \u201cCOHORT,\u201d \u201cCHARTS,\u201d \u201cGROUPS,\u201d and \u201cFILTER\u201d (Fig. 1B; Supplementary Tutorial). The portal begins in the \u201cCOHORT\u201d tab where users can select either the SJLIFE cohort, CCSS cohort, or both. The \u201cFILTER\u201d tab can then be used to refine the cohort by one or more variables in AND/OR combinations. For example, the cohort can be restricted to those survivors who were diag-nosed with cancer before age 5 years and who were exposed to either anthracycline chemotherapy or chest radiotherapy (Fig. 1B). In the \u201cGROUPS\u201d tab, users can divide the cohort into custom groups for comparative analysis (e.g., exposed group vs. nonexposed group). In the \u201cCHARTS\u201d tab, users can launch a set of features to explore, analyze, visualize, or export the data of the cohort/groups defined by the COHORT, FILTER, and GROUPS tabs. The \u201cdata dictionarancer survivorship data. Methods Data Sources The portal hosts data generated by the SJLIFE (December 2018 data freeze; refs. 9, 10) and the CCSS (January 2020 data freeze; refs. 11, 12). Both cohorts are retrospective cohorts with prospective fol-low-up of childhood cancer survivors who have survived at least 5 years following their diagnosis. Survivors in the SJLIFE cohort were diagnosed between 1962 and 2012 and treated at St. Jude Children\u2019s Research Hospital. Survivors in the CCSS cohort were diagnosed between 1970 and 1999 and treated at one of 31 pediatric oncology institutions in the US and Canada (Supplementary Table S1). Inclusion in the portal was determined as follows. For the SJLIFE cohort, all survivors with WGS data were included in the portal. Ad-ditionally, survivors without WGS but who visited the St. Jude cam-pus for clinical assessments were also included in the portal. For the CCSS cohort, survivors with WGS and who were not SJLIFE survivors were included in the\",\n    \"relevance_score\": 61,\n    \"sources\": [\n        {\n            \"document\": \"St. Jude Survivorship Portal\",\n            \"page\": 3,\n            \"start\": \"\",\n            \"stop\": \"\",\n            \"context\": \"ding individual survivor data for offline analysis. These features will enable on-demand access and open-ended exploration, allowing the St. Jude Survivorship Portal to serve as a valuable resource for the survivorship research community and beyond. Resul ts Overview of the Portal The St. Jude Survivorship Portal is a free, open-access data portal that hosts data from two childhood cancer survivor - ship cohorts: the SJLIFE and the CCSS (Fig. 1A; Supple- mentary Fig. S1; Supplementary Table S1). A total of 5,053 SJLIFE survivors and 2,688 CCSS survivors were included in the portal based on inclusion criteria described in \u201cMethods.\u201d Cohort phenotypic data consist of variables ranging from demographics, cancer diagnoses, treatments, clinical assess- ments, graded chronic health conditions, and self-reported symptoms of survivors with up to several decades of follow up (Supplementary Table S2). We have organized these data into a hierarchical data dictionary that users can easily n\",\n            \"distance\": 0.624\n        },\n        {\n            \"document\": \"St. Jude Survivorship Portal\",\n            \"page\": 3,\n            \"start\": \"\",\n            \"stop\": \"\",\n            \"context\": \"are avail- able through either the data dictionary or genome browser on the portal. Navigation through the St. Jude Survivorship Portal is guided by four main tabs: \u201cCOHORT,\u201d \u201cCHARTS,\u201d \u201cGROUPS,\u201d and \u201cFILTER\u201d (Fig. 1B; Supplementary Tutorial). The portal begins in the \u201cCOHORT\u201d tab where users can select either the SJLIFE cohort, CCSS cohort, or both. The \u201cFILTER\u201d tab can then be used to refine the cohort by one or more variables in AND/OR combinations. For example, the cohort can be restricted to those survivors who were diag-nosed with cancer before age 5 years and who were exposed to either anthracycline chemotherapy or chest radiotherapy (Fig. 1B). In the \u201cGROUPS\u201d tab, users can divide the cohort into custom groups for comparative analysis (e.g., exposed group vs. nonexposed group). In the \u201cCHARTS\u201d tab, users can launch a set of features to explore, analyze, visualize, or export the data of the cohort/groups defined by the COHORT, FILTER, and GROUPS tabs. The \u201cdata dictionar\",\n            \"distance\": 0.673\n        },\n        {\n            \"document\": \"St. Jude Survivorship Portal\",\n            \"page\": 11,\n            \"start\": \"\",\n            \"stop\": \"\",\n            \"context\": \"ancer survivorship data. Methods Data Sources The portal hosts data generated by the SJLIFE (December 2018 data freeze; refs. 9, 10) and the CCSS (January 2020 data freeze; refs. 11, 12). Both cohorts are retrospective cohorts with prospective fol-low-up of childhood cancer survivors who have survived at least 5 years following their diagnosis. Survivors in the SJLIFE cohort were diagnosed between 1962 and 2012 and treated at St. Jude Children\u2019s Research Hospital. Survivors in the CCSS cohort were diagnosed between 1970 and 1999 and treated at one of 31 pediatric oncology institutions in the US and Canada (Supplementary Table S1). Inclusion in the portal was determined as follows. For the SJLIFE cohort, all survivors with WGS data were included in the portal. Ad-ditionally, survivors without WGS but who visited the St. Jude cam-pus for clinical assessments were also included in the portal. For the CCSS cohort, survivors with WGS and who were not SJLIFE survivors were included in the\",\n            \"distance\": 0.716\n        }\n    ]\n}\n</code></pre>"},{"location":"development/#save-answer","title":"Save answer","text":"<p>This endpoint will save the answer generated by the LLM for user provided question. It will also give answer relevance score (ARS) and hallucination index (HI). The response will be in JSON format.</p> <p>The endpoint is:</p> <pre><code>POST /api/save_answer/\n</code></pre> <p>The request body will be:</p> <pre><code>{\n    \"question_text\": \"Which cohorts are available from Survivorship portal?\",\n    \"answer_text\": \"According to the text, the St. Jude Survivorship Portal hosts data from two childhood cancer survivor cohorts:\\n\\n1. The SJLIFE (St. Jude Lifetime) cohort: This cohort includes 5,053 survivors who were diagnosed between 1962 and 2012 and treated at St. Jude Children's Research Hospital.\\n2. The CCSS (Children's Oncology Group Childhood Cancer Survivor Study) cohort: This cohort includes 2,688 survivors who were diagnosed between 1970 and 1999 and treated at one of 31 pediatric oncology institutions in the US and Canada.\\n\\nBoth cohorts are retrospective cohorts with prospective follow-up of childhood cancer survivors who have survived at least 5 years following their diagnosis.\",\n    \"answer_no_context_text\": \"According to the Survivorship Portal, the following cohorts are available:\\n\\n1. Breast Cancer Cohort: This cohort includes women diagnosed with breast cancer who have completed treatment and are seeking information on survivorship and quality of life.\\n2. Colorectal Cancer Cohort: This cohort includes individuals diagnosed with colorectal cancer who have completed treatment and are looking for resources and support related to their diagnosis.\\n3. Lung Cancer Cohort: This cohort includes individuals diagnosed with lung cancer who have completed treatment and are seeking information on survivorship, quality of life, and managing late effects.\\n4. Ovarian Cancer Cohort: This cohort includes women diagnosed with ovarian cancer who have completed treatment and are looking for resources and support related to their diagnosis.\\n\\nPlease note that the availability of specific cohorts may vary depending on your location and other factors.\",\n    \"model_type\": \"llama3:latest\",\n    \"dataset\": \"Survivorship\",\n    \"no_context\": false,\n    \"use_default_ars\": true,\n    \"answer_best_distance\": 0.12,\n    \"answer_worst_distance\": 1.42,\n    \"use_default_hi\": true,\n    \"a_hi\": 1,\n    \"b_hi\": 0.5,\n    \"c_hi\": 0.5,\n    \"temperature\": 0.4,\n    \"top_k\": 20,\n    \"top_p\": 0.7\n}\n</code></pre> <p>The response will be:</p> <pre><code>{\n    \"saved\": true,\n    \"mean_distance_a\": 0.402,\n    \"relevance_score\": 73,\n    \"hallucination_index\": 22\n}\n</code></pre>"},{"location":"development/#get-question-details","title":"Get question details","text":"<p>This endpoint will return the details of the question asked by the user. The response will be in JSON format.</p> <p>The endpoint is:</p> <pre><code>GET /api/get_question_details/?question_id=1\n</code></pre> <p>The response will be:</p> <pre><code>{\n    \"question\": \"Is MyGPT another open-source GPT model?\",\n    \"relevance_score\": 66,\n    \"ground_truth\": \"-\",\n    \"question_type\": \"other\",\n    \"keywords\": \"\",\n    \"llm\": \"llama2:latest\",\n    \"answers\": [\n        {\n            \"answer\": \"No, MyGPT is not an open-source GPT model. According to the text, MyGPT is a complete, end-to-end pipeline built with open-source tools that are small enough for local installation on a laptop/personal computer or institutional servers/VM or cloud. The text also mentions that users can download any of the additional 90+ open-source LLMs available through Ollama via a command-line interface and use them in MyGPT. This suggests that MyGPT is a platform that leverages open-source LLMs, but is not itself an open-source GPT model.\",\n            \"relevance_score\": 49,\n            \"hallucination_index\": 43,\n            \"answer_no_context\": \"\\nNo, MyGPT is not another open-source GPT (Generative Pre-trained Transformer) model. While it is built on top of the transformer architecture and uses some of the same techniques as other open-source GPT models like BERT and RoBERTa, MyGPT has its own unique features and capabilities. MyGPT is designed to be more flexible and customizable than other GPT models, allowing users to fine-tune it for specific tasks and domains. Additionally, MyGPT includes a number of innovations and improvements over traditional GPT models, such as its use of a novel attention mechanism and its ability to handle long-range dependencies.\"\n        }\n    ],\n    \"sources\": [\n        {\n            \"paper\": \"MyGPT Manuscript\",\n            \"page\": 6,\n            \"context\": \"Users can also download any of the additional 90+ open-source LLMs available through Ollama via a command-line interface and use them in MyGPT. Ollama\u2019s performance depends on the available GPU, making access to GPU an essential component of running MyGPT. We tested several different model sizes to generate answers for different tasks. \",\n            \"distance\": 0.702\n        },\n        {\n            \"paper\": \"MyGPT Manuscript\",\n            \"page\": 13,\n            \"context\": \"13 Code availability All code, data, and tools are available at https://github.com/mb-group/MyGPT [This link is currently private. We have provided the source code as supplementary material for the review process. We have also provided a URL with username and password for each reviewer to the journal editor. Please contact them if you prefer to evaluate the hosted version of MyGPT]. Our code and data are licensed under GPL-3.0 license, while LLM models and LLM tools used for the project are available under the licenses of the respective sources. Author Contributions J.P. and M.M.B. conceived the MyGPT concept and the relevance metrics. J.P. and J.D. contributed equally to designing a prototype for MyGPT software. J.P. developed the MyGPT software and pipeline. B.M., V.T., and D.M. helped with multiple rounds of manual evaluation of the dataset. J.P. and J.D. performed automated evaluation with Ragas. D.M. helped with code review, S.A. \",\n            \"distance\": 0.711\n        },\n        {\n            \"paper\": \"MyGPT Manuscript\",\n            \"page\": 12,\n            \"context\": \"However, in contrast to the existing products, MyGPT is a complete, end-to-end pipeline built with open-source tools that are small enough for local installation on a laptop/personal computer or institutional servers/VM or cloud. MyGPT offers essential features \u2014 such as source surfacing, context highlighting, relevance scores, and hallucination index \u2014 while providing users complete control over their data by prioritizing privacy and customizations. We developed MyGPT to facilitate scientific research with academics in mind, but the ability to run it locally, to customize and optimize it for different use cases makes it invaluable to handle proprietary or confidential information in non-academic settings, such as hospitals, as well as for commercial and legal organizations (Figure 5B). \",\n            \"distance\": 0.799\n        },\n        {\n            \"paper\": \"MyGPT Figures\",\n            \"page\": 3,\n            \"context\": \"Figure 3: User interface for MyGPT. (A) MyGPT UI home screen. \",\n            \"distance\": 0.93\n        }\n    ]\n}\n</code></pre>"},{"location":"development/#get-conversation-history","title":"Get conversation history","text":"<p>This endpoint will return the conversation history of the user. The response will be in JSON format.</p> <p>The endpoint is:</p> <pre><code>GET /api/get_conversation_history/?dataset=GPCR\n</code></pre> <p>The response will be:</p> <pre><code>{\n    \"conversations\": [\n        {\n            \"conversation_id\": 6664,\n            \"questions_answers\": [\n                {\n                    \"question_id\": 7547,\n                    \"question\": \"What kind of analysis can be performed using Survivorship portal?\",\n                    \"relevance_score\": 58.0\n                }\n            ]\n        },\n        {\n            \"conversation_id\": 6299,\n            \"questions_answers\": [\n                {\n                    \"question_id\": 7100,\n                    \"question\": \"How many people participated in SJLife cohort? \",\n                    \"relevance_score\": 41.0\n                }\n            ]\n        },\n    ]\n}\n</code></pre>"},{"location":"development/#frontend-settings","title":"Frontend settings","text":"<p>This endpoint will return the frontend settings. The response will be in JSON format.</p> <p>The endpoint is:</p> <pre><code>GET /api/frontend_settings/\n</code></pre> <p>The response will be:</p> <pre><code>{\n    \"show_no_context_switch\": true,\n    \"restriction_without_login\": true,\n    \"azure_login\": true,\n    \"django_login\": false,\n    \"disable_chat_without_login\": false\n}\n</code></pre> <p>Here are the details of the settings:</p> <ul> <li><code>show_no_context_switch</code>: If true, the user can switch between RAG vs direct chat with LLM like ChatGPT.</li> <li><code>restriction_without_login</code>: If true, the user can't upload documents without logging in.</li> <li><code>azure_login</code>: If true, the user can login using Azure/Microsoft single sign-on (SSO).</li> <li><code>django_login</code>: If true, the user can login using Django authentication, which must be maintained sepretly by admin.</li> <li><code>disable_chat_without_login</code>: If true, the user can't chat with LLM without logging in.</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>MyGPT can be installed on following environments:</p> <ul> <li>Personal Computer</li> <li>Server/VM with GPU</li> <li>Cloud services (Azure)</li> </ul>"},{"location":"installation/#personal-computer","title":"Personal Computer","text":"<p>MyGPT is using Ollama for LLM server, and it requires at least 8GB (16GB for better response time) of RAM and 10GB of disk space. Also, Ollama is providing direct installation on Mac and Linux only. For Windows users we will use Docker to run Ollama.</p> <p>To run the pipleine on following environments, follow the instructions: * Mac     - Basic Installation</p> <ul> <li> <p>Linux</p> <ul> <li>Basic Installation</li> </ul> </li> <li> <p>Windows </p> <ul> <li>Basic Installation</li> </ul> <p>These instructions are simple and easy to follow. You can also modify bash scripts as per your convenience.</p> </li> </ul>"},{"location":"installation/#server-or-vm-with-gpu","title":"Server or VM with GPU","text":"<p>MyGPT can be hosted on a server or VM with GPU. For this installation we recommand to host User interface (UI), Backend server and Ollama (LLM server) on 3 seperate VMs. The Ollama VM should have a GPU with CUDA installed on the server/VM.</p> <p>To run the pipleine on VM/Server, follow the instructions: * Linux Server Installation</p>"},{"location":"installation/#cloud-services-azure","title":"Cloud services (Azure)","text":"<p>MyGPT can be hosted on any cloud service but we are providing Azure as an example deploymnet. For this installation we recommand to host User interface (UI), Backend server and Ollama (LLM server) on 3 seperate VMs. The Ollama VM should have a GPU with CUDA installed on the VM.</p> <p>To run the pipleine on Azure, follow the instructions: * Azure Installation</p>"},{"location":"installation/#user-interface","title":"User Interface","text":"<p>MyGPT user interface will allow users to check the publcation library, ask questions, and get answers. The user interface is built using ReactJS.</p> <p>Here is an example of the user interface with question, answer, and source citing:</p> <p></p>"},{"location":"installation/#mygpt-mcp-server-and-client","title":"MyGPT MCP Server and Client","text":"<p>MyGPT provides a basic implementation of the Model Context Protocol (MCP) server and client, facilitating integration with other private LLM applications and data sources within the same AI ecosystem. The MyGPT MCP server exposes data from the MyGPT backend using MCP tools, which any third-party MCP client can utilize. Conversely, the MyGPT MCP client enables connections to the MCP server, whether it is running locally or in a private network. This feature allows the client to use private database information as context, in addition to information from documents.</p> <p>[!NOTE]  To configure the MyGPT MCP server or client, follow the instructions on Readme file in the respective operating system.</p> <p></p> <p>MyGPT MCP Client User Interface (UI) and Customization Menu </p>"}]}